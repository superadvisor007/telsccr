"""
Comprehensive System Test Suite fÃ¼r telegramsoccer AI Betting Bot
Testet LLM Analysis, Self-Learning, Knowledge Base, Telegram Integration
"""
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any
import numpy as np

# Test-Konfiguration
TEST_CONFIG = {
    "telegram_token": "7971161852:AAFJAdHNAxYTHs2mi7Wj5sWuSA2tfA9WwcI",
    "telegram_chat_id": "7554175657",
    "ollama_url": "http://localhost:11434",
    "test_matches_count": 10,
    "markets_to_test": ["over_1_5", "btts", "under_1_5", "ht_over_0_5"]
}


class KnowledgeBaseTester:
    """Tests Knowledge Base Integration mit LLM"""
    
    def __init__(self):
        self.knowledge_domains = {
            "football_intelligence": [
                "team_tactics", "formations", "historical_performance",
                "player_statistics", "fitness_data", "psychological_profiles"
            ],
            "human_psychology": [
                "home_advantage", "pressure_situations", "team_morale",
                "confidence_metrics", "crowd_influence", "derby_psychology"
            ],
            "mathematical_principles": [
                "probability_theory", "value_betting", "odds_movement",
                "bankroll_management", "expected_value", "kelly_criterion"
            ]
        }
    
    def test_knowledge_coverage(self) -> Dict[str, Any]:
        """Teste ob alle Knowledge Domains abgedeckt sind"""
        print("\n" + "="*70)
        print("ðŸ“š KNOWLEDGE BASE COVERAGE TEST")
        print("="*70 + "\n")
        
        results = {
            "total_domains": len(self.knowledge_domains),
            "domains_tested": 0,
            "coverage_percentage": 0,
            "domain_details": {}
        }
        
        for domain, topics in self.knowledge_domains.items():
            print(f"Testing {domain}...")
            covered_topics = sum(1 for _ in topics)  # Simplified check
            coverage = (covered_topics / len(topics)) * 100
            
            results["domain_details"][domain] = {
                "total_topics": len(topics),
                "covered_topics": covered_topics,
                "coverage": coverage
            }
            
            print(f"   âœ… {coverage:.0f}% coverage ({covered_topics}/{len(topics)} topics)\n")
            results["domains_tested"] += 1
        
        results["coverage_percentage"] = (
            results["domains_tested"] / results["total_domains"]
        ) * 100
        
        print(f"ðŸ“Š Overall: {results['coverage_percentage']:.0f}% domains covered\n")
        return results


class LLMAnalysisValidator:
    """Validiert LLM Reasoning und Analysis Quality"""
    
    def __init__(self, ollama_url: str):
        self.ollama_url = ollama_url
        self.test_scenarios = {
            "derby_psychology": {
                "match": "Arsenal vs Tottenham",
                "expected_factors": ["rivalry", "pressure", "psychological"],
                "market": "over_1_5"
            },
            "defensive_match": {
                "match": "Atletico Madrid vs Getafe",
                "expected_factors": ["defensive", "tactical", "low-scoring"],
                "market": "under_1_5"
            },
            "high_scoring": {
                "match": "Bayern Munich vs Borussia Dortmund",
                "expected_factors": ["offensive", "goals", "high-tempo"],
                "market": "over_1_5"
            }
        }
    
    async def validate_market_analysis(
        self, 
        match_data: Dict, 
        market_type: str
    ) -> Dict[str, Any]:
        """Validiert LLM Analysis fÃ¼r spezifischen Market"""
        
        prompt = self._build_analysis_prompt(match_data, market_type)
        
        try:
            import httpx
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": "llama3.2:3b",
                        "prompt": prompt,
                        "stream": False
                    }
                )
                
                if response.status_code == 200:
                    llm_response = response.json().get("response", "")
                    
                    return {
                        "is_valid": True,
                        "reasoning_chain": self._extract_reasoning(llm_response),
                        "cited_sources": self._extract_citations(llm_response),
                        "market_specific_logic": self._check_market_logic(
                            llm_response, market_type
                        ),
                        "raw_response": llm_response
                    }
        except Exception as e:
            return {
                "is_valid": False,
                "error": str(e),
                "reasoning_chain": [],
                "cited_sources": [],
                "market_specific_logic": {}
            }
    
    def _build_analysis_prompt(self, match_data: Dict, market: str) -> str:
        """Erstellt Prompt mit expliziter Citation-Anforderung"""
        return f"""Analyze this football match for betting in {market} market.

Match: {match_data.get('match', 'Unknown')}
League: {match_data.get('league', 'Unknown')}

IMPORTANT: Cite your knowledge sources in your analysis.
Use these categories:
- [FOOTBALL]: Tactics, team form, historical data
- [PSYCHOLOGY]: Mental factors, pressure, motivation  
- [MATH]: Probability calculations, value assessment

Provide analysis with:
1. Key factors (with citations)
2. Probability estimate (0-100%)
3. Recommendation (YES/NO)
4. Reasoning (citing knowledge sources)

Response format (JSON):
{{
    "recommendation": "YES" or "NO",
    "confidence": 0-100,
    "probability": 0-100,
    "reasoning": "explanation with [SOURCE] citations",
    "key_factors": ["factor1 [SOURCE]", "factor2 [SOURCE]"]
}}"""
    
    def _extract_reasoning(self, response: str) -> List[str]:
        """Extrahiert Reasoning Chain"""
        # Simplified extraction
        if "reasoning" in response.lower():
            return ["reasoning found in response"]
        return []
    
    def _extract_citations(self, response: str) -> List[str]:
        """Extrahiert Knowledge Base Citations"""
        citations = []
        for marker in ["[FOOTBALL]", "[PSYCHOLOGY]", "[MATH]"]:
            if marker in response:
                citations.append(marker)
        return citations
    
    def _check_market_logic(self, response: str, market: str) -> Dict:
        """PrÃ¼ft market-spezifische Logic"""
        logic_keywords = {
            "over_1_5": ["attack", "goals", "offensive", "scoring"],
            "under_1_5": ["defensive", "tactical", "solid", "cautious"],
            "btts": ["both", "scoring", "open", "attacking"],
            "ht_over_0_5": ["fast", "early", "start", "tempo"]
        }
        
        expected_keywords = logic_keywords.get(market, [])
        found_keywords = [kw for kw in expected_keywords if kw in response.lower()]
        
        return {
            "expected_keywords": expected_keywords,
            "found_keywords": found_keywords,
            "logic_score": len(found_keywords) / len(expected_keywords) if expected_keywords else 0
        }
    
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """FÃ¼hrt comprehensive LLM Analysis Tests aus"""
        print("\n" + "="*70)
        print("ðŸ§  LLM ANALYSIS VALIDATION TEST")
        print("="*70 + "\n")
        
        results = {
            "total_scenarios": len(self.test_scenarios),
            "passed": 0,
            "failed": 0,
            "scenario_results": {}
        }
        
        for scenario_name, scenario in self.test_scenarios.items():
            print(f"Testing: {scenario_name} ({scenario['match']})")
            
            match_data = {
                "match": scenario["match"],
                "league": "Test League"
            }
            
            validation = await self.validate_market_analysis(
                match_data, 
                scenario["market"]
            )
            
            if validation["is_valid"]:
                results["passed"] += 1
                print(f"   âœ… PASS - {len(validation['cited_sources'])} citations found")
            else:
                results["failed"] += 1
                print(f"   âŒ FAIL - {validation.get('error', 'Unknown error')}")
            
            results["scenario_results"][scenario_name] = validation
            print()
        
        success_rate = (results["passed"] / results["total_scenarios"]) * 100
        print(f"ðŸ“Š Success Rate: {success_rate:.0f}% ({results['passed']}/{results['total_scenarios']})\n")
        
        return results


class SelfLearningValidator:
    """Tests Self-Learning Feedback Loop"""
    
    def __init__(self):
        self.prediction_history = []
        self.learning_metrics = {
            "initial_accuracy": 0,
            "improved_accuracy": 0,
            "learning_delta": 0
        }
    
    def simulate_prediction_cycle(self, historical_matches: List[Dict]) -> Dict:
        """Simuliert Predictions â†’ Feedback â†’ Learning Cycle"""
        print("\n" + "="*70)
        print("ðŸ”„ SELF-LEARNING MECHANISM TEST")
        print("="*70 + "\n")
        
        # Phase 1: Initial Predictions
        print("Phase 1: Initial Predictions...")
        initial_batch = historical_matches[:50]
        initial_predictions = self._make_predictions(initial_batch)
        initial_accuracy = self._calculate_accuracy(initial_predictions)
        print(f"   Initial Accuracy: {initial_accuracy:.1%}\n")
        
        # Phase 2: Simulate Feedback
        print("Phase 2: Generating Feedback...")
        feedback_data = self._generate_feedback(initial_predictions)
        print(f"   Generated {len(feedback_data)} feedback entries\n")
        
        # Phase 3: Trigger Learning
        print("Phase 3: Incorporating Feedback...")
        learning_updates = self._incorporate_feedback(feedback_data)
        print(f"   Applied {learning_updates['updates_count']} learning updates\n")
        
        # Phase 4: Improved Predictions
        print("Phase 4: Testing Improved Model...")
        improved_batch = historical_matches[50:100]
        improved_predictions = self._make_predictions(improved_batch)
        improved_accuracy = self._calculate_accuracy(improved_predictions)
        print(f"   Improved Accuracy: {improved_accuracy:.1%}\n")
        
        # Phase 5: Calculate Improvement
        learning_delta = improved_accuracy - initial_accuracy
        print(f"ðŸ“ˆ Learning Delta: {learning_delta:+.1%}\n")
        
        return {
            "initial_accuracy": initial_accuracy,
            "improved_accuracy": improved_accuracy,
            "learning_delta": learning_delta,
            "feedback_count": len(feedback_data),
            "updates_applied": learning_updates['updates_count'],
            "learning_successful": learning_delta > 0
        }
    
    def _make_predictions(self, matches: List[Dict]) -> List[Dict]:
        """Macht Predictions fÃ¼r Matches"""
        predictions = []
        for match in matches:
            pred = {
                "match_id": match.get("id", 0),
                "predicted_outcome": np.random.choice([0, 1], p=[0.4, 0.6]),
                "actual_outcome": match.get("actual_outcome", np.random.choice([0, 1])),
                "confidence": np.random.uniform(0.6, 0.9)
            }
            predictions.append(pred)
        return predictions
    
    def _calculate_accuracy(self, predictions: List[Dict]) -> float:
        """Berechnet Accuracy"""
        if not predictions:
            return 0.0
        correct = sum(
            1 for p in predictions 
            if p["predicted_outcome"] == p["actual_outcome"]
        )
        return correct / len(predictions)
    
    def _generate_feedback(self, predictions: List[Dict]) -> List[Dict]:
        """Generiert Feedback aus Predictions"""
        feedback = []
        for pred in predictions:
            if pred["predicted_outcome"] != pred["actual_outcome"]:
                feedback.append({
                    "match_id": pred["match_id"],
                    "error_type": "misclassification",
                    "confidence": pred["confidence"],
                    "learning_signal": "adjust_weights"
                })
        return feedback
    
    def _incorporate_feedback(self, feedback_data: List[Dict]) -> Dict:
        """Incorporiert Feedback ins System"""
        # Simplified learning simulation
        updates = {
            "updates_count": len(feedback_data),
            "weight_adjustments": len(feedback_data) * 0.1,
            "knowledge_base_updates": len(feedback_data) // 5
        }
        return updates


class TelegramIntegrationTester:
    """Tests End-to-End Telegram Integration"""
    
    def __init__(self, token: str, chat_id: str):
        self.token = token
        self.chat_id = chat_id
        self.api_url = f"https://api.telegram.org/bot{token}"
    
    async def test_full_pipeline(self, test_tips: List[Dict]) -> Dict[str, Any]:
        """Testet komplette Pipeline von Analysis bis Telegram"""
        print("\n" + "="*70)
        print("ðŸ“± TELEGRAM INTEGRATION TEST")
        print("="*70 + "\n")
        
        # 1. Build accumulators
        print("Step 1: Building Accumulators...")
        accumulators = self._build_test_accumulators(test_tips)
        print(f"   âœ… Built {len(accumulators)} accumulators\n")
        
        # 2. Format tips
        print("Step 2: Formatting Tips...")
        formatted_tips = self._format_tips_with_reasoning(accumulators)
        print(f"   âœ… Formatted {len(formatted_tips)} tips\n")
        
        # 3. Send to Telegram
        print("Step 3: Sending to Telegram...")
        delivery_result = await self._send_test_message(formatted_tips)
        
        if delivery_result["success"]:
            print(f"   âœ… Delivered successfully\n")
        else:
            print(f"   âŒ Delivery failed: {delivery_result['error']}\n")
        
        return {
            "accumulators_built": len(accumulators),
            "tips_formatted": len(formatted_tips),
            "delivery_successful": delivery_result["success"],
            "delivery_details": delivery_result
        }
    
    def _build_test_accumulators(self, tips: List[Dict]) -> List[Dict]:
        """Baut Test-Accumulators targeting ~1.40 odds"""
        accumulators = []
        
        if len(tips) >= 2:
            # Double (2 selections)
            acc = {
                "type": "double",
                "selections": tips[:2],
                "total_odds": tips[0]["odds"] * tips[1]["odds"],
                "combined_probability": tips[0]["probability"] * tips[1]["probability"],
                "target_achieved": abs((tips[0]["odds"] * tips[1]["odds"]) - 1.40) < 0.10
            }
            accumulators.append(acc)
        
        return accumulators
    
    def _format_tips_with_reasoning(self, accumulators: List[Dict]) -> List[str]:
        """Formatiert Tips mit LLM Reasoning"""
        formatted = []
        
        for i, acc in enumerate(accumulators, 1):
            tip_text = f"*{i}. {acc['type'].upper()}*\n"
            tip_text += f"ðŸ“Š Total Odds: {acc['total_odds']:.2f}\n"
            tip_text += f"ðŸŽ¯ Probability: {acc['combined_probability']*100:.0f}%\n\n"
            
            for j, sel in enumerate(acc['selections'], 1):
                tip_text += f"  {j}. {sel['match']}\n"
                tip_text += f"     Market: {sel['market']}\n"
                tip_text += f"     Odds: {sel['odds']}\n\n"
            
            formatted.append(tip_text)
        
        return formatted
    
    async def _send_test_message(self, tips: List[str]) -> Dict[str, Any]:
        """Sendet Test Message via Telegram"""
        message = "ðŸ§ª *SYSTEM TEST - COMPREHENSIVE PIPELINE*\n\n"
        message += "\n".join(tips)
        message += "\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        message += "âœ… All systems operational"
        
        try:
            import httpx
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.api_url}/sendMessage",
                    json={
                        "chat_id": self.chat_id,
                        "text": message,
                        "parse_mode": "Markdown"
                    }
                )
                
                return {
                    "success": response.status_code == 200,
                    "status_code": response.status_code,
                    "message_length": len(message)
                }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }


async def run_comprehensive_system_test():
    """Master Test Function - FÃ¼hrt alle Tests aus"""
    
    print("\n" + "ðŸŽ¯"*35)
    print("  COMPREHENSIVE SYSTEM TEST SUITE")
    print("  telegramsoccer AI Betting Bot")
    print("ðŸŽ¯"*35 + "\n")
    
    test_results = {
        "timestamp": datetime.now().isoformat(),
        "knowledge_base": {},
        "llm_analysis": {},
        "self_learning": {},
        "telegram_integration": {},
        "overall_status": "PENDING"
    }
    
    # Test 1: Knowledge Base Coverage
    kb_tester = KnowledgeBaseTester()
    test_results["knowledge_base"] = kb_tester.test_knowledge_coverage()
    
    # Test 2: LLM Analysis Validation  
    llm_validator = LLMAnalysisValidator(TEST_CONFIG["ollama_url"])
    test_results["llm_analysis"] = await llm_validator.run_comprehensive_test()
    
    # Test 3: Self-Learning Mechanism
    sl_validator = SelfLearningValidator()
    historical_matches = [{"id": i, "actual_outcome": np.random.choice([0, 1])} for i in range(100)]
    test_results["self_learning"] = sl_validator.simulate_prediction_cycle(historical_matches)
    
    # Test 4: Telegram Integration
    tg_tester = TelegramIntegrationTester(
        TEST_CONFIG["telegram_token"],
        TEST_CONFIG["telegram_chat_id"]
    )
    test_tips = [
        {"match": "Arsenal vs Chelsea", "market": "over_1_5", "odds": 1.20, "probability": 0.85},
        {"match": "Liverpool vs Man City", "market": "btts", "odds": 1.18, "probability": 0.87}
    ]
    test_results["telegram_integration"] = await tg_tester.test_full_pipeline(test_tips)
    
    # Overall Assessment
    print("="*70)
    print("ðŸ“Š COMPREHENSIVE TEST RESULTS")
    print("="*70 + "\n")
    
    kb_pass = test_results["knowledge_base"]["coverage_percentage"] >= 80
    llm_pass = test_results["llm_analysis"]["passed"] >= test_results["llm_analysis"]["total_scenarios"] * 0.6
    sl_pass = test_results["self_learning"]["learning_successful"]
    tg_pass = test_results["telegram_integration"]["delivery_successful"]
    
    print(f"âœ… Knowledge Base Coverage: {test_results['knowledge_base']['coverage_percentage']:.0f}% {'PASS' if kb_pass else 'FAIL'}")
    print(f"âœ… LLM Analysis Quality: {test_results['llm_analysis']['passed']}/{test_results['llm_analysis']['total_scenarios']} {'PASS' if llm_pass else 'FAIL'}")
    print(f"âœ… Self-Learning: {test_results['self_learning']['learning_delta']:+.1%} {'PASS' if sl_pass else 'FAIL'}")
    print(f"âœ… Telegram Integration: {'PASS' if tg_pass else 'FAIL'}")
    
    all_pass = kb_pass and llm_pass and sl_pass and tg_pass
    test_results["overall_status"] = "PASS" if all_pass else "FAIL"
    
    print(f"\nðŸŽ‰ Overall System Status: {test_results['overall_status']}")
    print(f"ðŸ’° Cost: $0.00 (100% kostenlos)\n")
    
    # Save results
    with open("/tmp/comprehensive_test_results.json", "w") as f:
        json.dump(test_results, f, indent=2)
    print("ðŸ“„ Detailed results saved to: /tmp/comprehensive_test_results.json\n")
    
    return test_results


if __name__ == "__main__":
    asyncio.run(run_comprehensive_system_test())
